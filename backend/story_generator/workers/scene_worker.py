"""
Scene Worker - Worker táº¡o scenes trong background. 

NHIá»†M Vá»¤: 
- Nháº­n scenes 2-6 tá»« API
- Táº¡o PARALLEL 3 scenes cÃ¹ng lÃºc (batch processing)
- Upload lÃªn storage
- Update database
- Update progress sau má»—i batch
"""

import asyncio
import logging
import time
from story_generator.database import Database
from story_generator.services.image_generator import ImageGenerator
from story_generator.services.voice_generator import VoiceGenerator 

logger = logging.getLogger(__name__)


async def generate_single_scene_worker(
    scene_data: dict,
    db_scene: dict,
    request_params: dict,
    character_design: str,
    background_design: str,
    story_id: str,
    image_gen: ImageGenerator,
    voice_gen: VoiceGenerator,
    db: Database
) -> dict:
    """
    Táº¡o Má»˜T scene (image + audio).
    
    DÃ¹ng trong worker Ä‘á»ƒ process parallel.
    
    Returns:
        dict vá»›i status ('completed' hoáº·c 'failed')
    """
    scene_num = db_scene["scene_order"]
    scene_id = db_scene["id"]
    
    # âœ… THÃŠM:  Timing variables
    total_start = time.time()
    try:
        #logger.info(f"   ğŸ¨ Scene {scene_num} starting...")      
        #1. Update status = generating
        await db.update_scene_status(scene_id, "generating")
        
        gen_start = time.time()
        
        #2. Generate image + audio (parallel)
        image_task = image_gen.generate_image(
            prompt=db_scene["image_prompt_used"],
            style=request_params.get("image_style"),
            scene_number=scene_num,
            character_design=character_design,
            background_design=background_design
        )
        
        audio_task = voice_gen.generate_audio(
            text=db_scene["paragraph_text"],
            voice=request_params.get("voice")
        )
        
        image_bytes, (audio_bytes, audio_duration, transcript) = await asyncio.gather(
            image_task,
            audio_task
        )
        
        gen_end = time.time()
        gen_time = gen_end - gen_start

        #3. Upload (parallel)
        upload_start = time.time()

        image_path = f"{story_id}/scene_{scene_num}.webp"
        audio_path = f"{story_id}/scene_{scene_num}.mp3"
        
        image_url, audio_url = await asyncio.gather(
            db.upload_file("story-images", image_path, image_bytes, "image/webp"),
            db.upload_file("story-audio", audio_path, audio_bytes, "audio/mpeg")
        )

        upload_end = time.time()
        upload_time = upload_end - upload_start
        #4. Update scene database and Ä‘Ã¡nh dáº¥u Success
        await asyncio.gather(
            db.update_scene(scene_id, {
                "image_url":  image_url,
                "audio_url": audio_url,
                "transcript": transcript,
                "audio_duration": audio_duration, 
            }),
            db.update_scene_status(scene_id, "completed")
        )
        
        # Update status = completed
        #await db.update_scene_status(scene_id, "completed")
        total_duration = gen_time + upload_time

        # âœ… LOG DETAILED SUMMARY
        logger.info(f"")
        logger.info(f"â±ï¸  SCENE {scene_num}:")
        logger.info(f"   â€¢ Generation (image+audio): {gen_time:.2f}s")
        logger.info(f"   â€¢ Upload:                    {upload_time:.2f}s")
        logger.info(f"   â€¢ Total:                     {total_duration:.2f}s")
        logger.info(f"")
        
        return {
            "scene_number": scene_num,
            "scene_id": scene_id,
            "status": "completed",
            "duration": total_duration,
            "timings": {
                "generation":  round(gen_time, 2),
                "upload": round(upload_time, 2)
            }
        }
        
    except Exception as e:
        logger.error(f"   âŒ Scene {scene_num} FAILED: {e}")
        
        # Mark as failed
        await db.update_scene_status(
            scene_id, 
            "failed", 
            error_message=str(e)
        )
        
        return {
            "scene_number":  scene_num,
            "scene_id": scene_id,
            "status": "failed",
            "error":  str(e)
        }


async def generate_remaining_scenes(
    story_id: str,
    scenes_data: list,
    db_scenes: list,
    request_params: dict,
    character_design: str,
    background_design: str,
    image_gen: ImageGenerator,
    voice_gen: VoiceGenerator,
    db: Database,
    story_start_time: float = None
):
    """
    Worker function - Táº¡o scenes 2-6 vá»›i PARALLEL PROCESSING.
    
    STRATEGY:
    - Táº¡o 3 scenes cÃ¹ng lÃºc (batch)
    - Batch 1: scenes 2, 3, 4
    - Batch 2: scenes 5, 6
    
    BENEFITS:
    - Giáº£m thá»i gian tá»« ~30s xuá»‘ng ~15s cho 6 scenes
    - Táº­n dá»¥ng I/O concurrency
    - API calls song song
    
    Args:
        story_id: ID cá»§a story
        scenes_data: List data tá»« Gemini
        db_scenes: List scene records tá»« DB
        request_params: Parameters (image_style, voice)
        character_design: Character description
        background_design: Background description
        image_gen: ImageGenerator instance
        voice_gen: VoiceGenerator instance
        db: Database instance
    """
    worker_start_time = time.time()
    logger.info(f"ğŸš€ Worker started: {story_id}")
    #logger.info(f"   Strategy:  Parallel batch processing (3 scenes per batch)")
    
    total_scenes = len(db_scenes)
    completed_count = 1  # Scene 1 Ä‘Ã£ xong
    
    try:
        # Skip scene 1 (Ä‘Ã£ táº¡o trong API 1)
        remaining = list(zip(scenes_data[1:], db_scenes[1:]))
        
        if not remaining:
            logger.warning("âš ï¸ No remaining scenes to generate")
            return
        
        # ==========================================
        # PARALLEL BATCH PROCESSING
        # ==========================================
        BATCH_SIZE = 5  # Táº¡o 5 scenes cÃ¹ng lÃºc
        
        # Chia thÃ nh batches
        batches = [
            remaining[i:i + BATCH_SIZE] 
            for i in range(0, len(remaining), BATCH_SIZE)
        ]
        
        logger.info(f"   ğŸ“¦ Total batches:   {len(batches)}")
        
        for batch_idx, batch in enumerate(batches, 1):
            batch_size = len(batch)
            scene_numbers = [db_scene["scene_order"] for _, db_scene in batch]
            
            logger.info(f"")
            logger.info(f"ğŸ“¦ BATCH {batch_idx}/{len(batches)}: Scenes {scene_numbers}")
            #logger.info(f"   Processing {batch_size} scenes in parallel...")
            
            batch_start = time.time()
            
            # ==========================================
            # Táº O Táº¤T Cáº¢ SCENES TRONG BATCH SONG SONG
            # ==========================================
            tasks = []
            for scene_data, db_scene in batch: 
                task = generate_single_scene_worker(
                    scene_data=scene_data,
                    db_scene=db_scene,
                    request_params=request_params,
                    character_design=character_design,
                    background_design=background_design,
                    story_id=story_id,
                    image_gen=image_gen,
                    voice_gen=voice_gen,
                    db=db
                )
                tasks.append(task)
            
            # Äá»£i Táº¤T Cáº¢ scenes trong batch hoÃ n thÃ nh
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            batch_duration = time.time() - batch_start
            
            # ==========================================
            # PROCESS RESULTS
            # ==========================================
            completed_in_batch = 0
            failed_in_batch = 0
            
            for result in results: 
                if isinstance(result, Exception):
                    logger.error(f"   âŒ Task exception: {result}")
                    failed_in_batch += 1
                elif result["status"] == "completed":
                    completed_in_batch += 1
                else:
                    failed_in_batch += 1
            
            # Update total completed count
            completed_count += completed_in_batch + failed_in_batch
            
            # ==========================================
            # UPDATE STORY PROGRESS
            # ==========================================
            await db.update_story_progress(story_id, completed_count, total_scenes)
            
            # logger.info(f"")
            # logger.info(f"âœ… BATCH {batch_idx} DONE in {batch_duration:.2f}s")
            # logger.info(f"   Completed: {completed_in_batch}/{batch_size}")
            # logger.info(f"   Failed: {failed_in_batch}/{batch_size}")
            # logger.info(f"   Overall progress: {completed_count}/{total_scenes}")
        
        # ==========================================
        # ALL BATCHES COMPLETED
        # ==========================================
        await db.update_story_status(story_id, "completed")
        
        logger.info(f"")
        logger.info(f"ğŸ‰ Story {story_id} FULLY COMPLETED!")
        logger.info(f"   Total scenes: {completed_count}/{total_scenes}")
        
        if story_start_time:
            grand_total_time = time.time() - story_start_time
            logger.info(f"â±ï¸  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            logger.info(f"â±ï¸  ğŸ GRAND TOTAL TIME: {grand_total_time:.2f}s")
            logger.info(f"â±ï¸     (From request start to all scenes completed)")
            logger.info(f"â±ï¸  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            logger.info(f"")
            
    except Exception as e:
        logger.error(f"âŒ Worker CRITICAL FAILURE [{story_id}]: {e}", exc_info=True)
        
        # Mark story as failed
        await db.update_story_status(story_id, "failed")
        
        try:
            await db.client. table("stories").update({
                "error_message": str(e)
            }).eq("id", story_id).execute()
        except:
            pass